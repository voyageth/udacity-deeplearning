{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LecunLCN(X, image_shape, threshold=1e-4, radius=7, use_divisor=True):\n",
    "    \"\"\"Local Contrast Normalization\"\"\"\n",
    "    \"\"\"[http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf]\"\"\"\n",
    "\n",
    "    # Get Gaussian filter\n",
    "    filter_shape = (radius, radius, image_shape[3], 1)\n",
    "\n",
    "    #self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)\n",
    "    filters = gaussian_filter(filter_shape)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    # Compute the Guassian weighted average by means of convolution\n",
    "    convout = tf.nn.conv2d(X, filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "    # Subtractive step\n",
    "    mid = int(np.floor(filter_shape[1] / 2.))\n",
    "\n",
    "    # Make filter dimension broadcastable and subtract\n",
    "    centered_X = tf.sub(X, convout)\n",
    "\n",
    "    # Boolean marks whether or not to perform divisive step\n",
    "    if use_divisor:\n",
    "        # Note that the local variances can be computed by using the centered_X\n",
    "        # tensor. If we convolve this with the mean filter, that should give us\n",
    "        # the variance at each point. We simply take the square root to get our\n",
    "        # denominator\n",
    "\n",
    "        # Compute variances\n",
    "        sum_sqr_XX = tf.nn.conv2d(tf.square(centered_X), filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "        # Take square root to get local standard deviation\n",
    "        denom = tf.sqrt(sum_sqr_XX)\n",
    "\n",
    "        per_img_mean = tf.reduce_mean(denom)\n",
    "        divisor = tf.maximum(per_img_mean, denom)\n",
    "        # Divisise step\n",
    "        new_X = tf.truediv(centered_X, tf.maximum(divisor, threshold))\n",
    "    else:\n",
    "        new_X = centered_X\n",
    "\n",
    "    return new_X\n",
    "\n",
    "\n",
    "def gaussian_filter(kernel_shape):\n",
    "    x = np.zeros(kernel_shape, dtype = float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    \n",
    "    for kernel_idx in xrange(0, kernel_shape[2]):\n",
    "        for i in xrange(0, kernel_shape[0]):\n",
    "            for j in xrange(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gauss(i - mid, j - mid)\n",
    "    \n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "def gauss(x, y, sigma=3.0):\n",
    "    Z = 2 * np.pi * sigma ** 2\n",
    "    return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from multi_digit_mnist_data import MultiDigitMNISTData\n",
    "from multi_digit_svhn_data import MultiDigitSVHNData\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "class MultiDigitModel(object):\n",
    "    def __init__(self, multi_digit_dataset, digit_count=1, num_channels=1, pooling_stride=2, num_steps=10001,\n",
    "                 batch_size=128, patch_size=5, num_convs=None, num_fc_1=1600, num_fc_2=512, beta=0.001,\n",
    "                 drop_out_rate=0.5, learning_rate_start=0.1, learning_rate_decay_rate=0.1, add_l2_loss=False):\n",
    "        if num_convs is None:\n",
    "            num_convs = [24, 32, 32, 32, 32, 32, 32, 64]\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "        self.pooling_stride = pooling_stride\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_conv_1 = num_convs[0]\n",
    "        self.num_conv_2 = num_convs[1]\n",
    "        self.num_conv_3 = num_convs[2]\n",
    "        self.num_conv_4 = num_convs[3]\n",
    "        self.num_conv_5 = num_convs[4]\n",
    "        self.num_conv_6 = num_convs[5]\n",
    "        self.num_conv_7 = num_convs[6]\n",
    "        self.num_conv_8 = num_convs[7]\n",
    "\n",
    "        self.last_num_conv = 0\n",
    "        for i in range(7, -1, -1):\n",
    "            if num_convs[i] > 0:\n",
    "                self.last_num_conv = num_convs[i]\n",
    "                break\n",
    "\n",
    "        self.num_fc_1 = num_fc_1\n",
    "        self.num_fc_2 = num_fc_2\n",
    "\n",
    "        self.beta = beta\n",
    "        self.drop_out_rate = drop_out_rate\n",
    "        self.learning_rate_start = learning_rate_start\n",
    "        self.learning_rate_decay_rate = learning_rate_decay_rate\n",
    "\n",
    "        self.digit_count = digit_count\n",
    "        self.multi_digit_dataset = multi_digit_dataset\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        self.add_l2_loss = add_l2_loss\n",
    "        self.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "        self.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "        # Constants describing the training process.\n",
    "        self.MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "        self.NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_digit(p_length, p_digits,\n",
    "                       batch_length_labels, batch_digits_labels, digit):\n",
    "        eq_count = 0.0\n",
    "        total_count = 0.0\n",
    "        for i in range(0, len(p_digits[digit])):\n",
    "            if np.argmax(batch_length_labels[i]) >= digit:\n",
    "                total_count += 1.0\n",
    "                if np.argmax(p_digits[digit][i]) == np.argmax(batch_digits_labels[i][digit]):\n",
    "                    eq_count += 1.0\n",
    "                    #         elif digit == 1:\n",
    "                    #           print(\"np.argmax(p_digits[digit][i]):{}, np.argmax(batch_digits_labels[i][digit]:{}\".format(np.argmax(p_digits[digit][i]), np.argmax(batch_digits_labels[i][digit])))\n",
    "\n",
    "        if total_count == 0:\n",
    "            return 0\n",
    "        return eq_count / total_count * 100\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4,\n",
    "                        batch_length_labels, batch_digits_labels):\n",
    "        eq_count = 0.0\n",
    "        for i in range(0, len(p_length)):\n",
    "            if np.argmax(p_length[i]) == np.argmax(batch_length_labels[i]):\n",
    "                eq_count += 1.0\n",
    "        return eq_count / len(p_length) * 100\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(p_length, p_digits,\n",
    "                 batch_length_labels, batch_digits_labels):\n",
    "        eq_count = 0.0\n",
    "        for row_index in range(0, len(p_length)):\n",
    "            # print(\"row_index:{}\".format(row_index))\n",
    "            one_based_length_predicted = np.argmax(p_length[row_index])\n",
    "            one_based_length_real = np.argmax(batch_length_labels[row_index])\n",
    "\n",
    "            # print(\"one_based_length_predicted : {}, one_based_length_real :{}\".format(one_based_length_predicted, one_based_length_real))\n",
    "            if one_based_length_predicted == one_based_length_real:\n",
    "                is_equal = True\n",
    "                for digit_index in range(0, one_based_length_real):\n",
    "                    # print(\"\\tdigit_index:{}\".format(digit_index))\n",
    "                    if np.argmax(p_digits[digit_index][row_index]) != np.argmax(\n",
    "                            batch_digits_labels[row_index][digit_index]):\n",
    "                        # print(\"\\t\\tnp.argmax(p_digits[digit_index][row_index]) :{}, np.argmax(batch_digits_labels[row_index][digit_index]) :{}\".format(np.argmax(p_digits[digit_index][row_index]), np.argmax(batch_digits_labels[row_index][digit_index])))\n",
    "                        is_equal = False\n",
    "                        break\n",
    "                if is_equal:\n",
    "                    eq_count += 1.0\n",
    "\n",
    "        return eq_count / len(p_length) * 100\n",
    "    \n",
    "    \n",
    "    def _activation_summary(self, x):\n",
    "        tf.histogram_summary(x.op.name + '/activations', x)\n",
    "        tf.scalar_summary(x.op.name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "    def _add_loss_summaries(self, total_loss):\n",
    "        # Compute the moving average of all individual losses and the total loss.\n",
    "        loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "        losses = tf.get_collection('losses')\n",
    "        loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "        # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "        # same for the averaged version of the losses.\n",
    "        for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "            tf.scalar_summary(l.op.name +' (raw)', l)\n",
    "            tf.scalar_summary(l.op.name, loss_averages.average(l))\n",
    "\n",
    "        return loss_averages_op\n",
    "\n",
    "        \n",
    "    def _variable_on_cpu(self, name, shape, initializer):\n",
    "        with tf.device('/cpu:0'):\n",
    "            var = tf.get_variable(name, shape, initializer=tf.contrib.layers.xavier_initializer_conv2d(), dtype=tf.float32)\n",
    "        return var\n",
    "        \n",
    "        \n",
    "    def _variable_with_weight_decay(self, name, shape, stddev, wd):\n",
    "        var = self._variable_on_cpu(\n",
    "            name,\n",
    "            shape,\n",
    "            tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        )\n",
    "        if wd is not None:\n",
    "            weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "        return var\n",
    "    \n",
    "    def conv_layer(self, number, input_data, input_num, output_num):\n",
    "        with tf.variable_scope('conv' + str(number)) as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                             shape=[self.patch_size, self.patch_size, input_num, output_num],\n",
    "                             stddev=5e-2,\n",
    "                             wd=0.0)\n",
    "            conv = tf.nn.conv2d(input_data, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [output_num], tf.constant_initializer(0.0))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv)\n",
    "#         norm = tf.nn.lrn(conv, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm' + str(number))\n",
    "#         return norm\n",
    "        return conv\n",
    "    \n",
    "    # inference.\n",
    "    def inference(self, data, is_training=True):\n",
    "        stride = 2\n",
    "        data=LecunLCN(data,[self.batch_size, self.multi_digit_dataset.image_width, self.multi_digit_dataset.image_height, self.num_channels])\n",
    "        conv = None\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                             shape=[self.patch_size, self.patch_size, self.num_channels, self.num_conv_1],\n",
    "                             stddev=5e-2,\n",
    "                             wd=0.0)\n",
    "            conv = tf.nn.conv2d(data, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [self.num_conv_1], tf.constant_initializer(0.0))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv1)\n",
    "        pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], [1, stride, stride, 1], padding='SAME', name='pool1')\n",
    "#         norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "        conv = pool1\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            kernel = self._variable_with_weight_decay('weights',\n",
    "                             shape=[self.patch_size, self.patch_size, self.num_conv_1, self.num_conv_2],\n",
    "                             stddev=5e-2,\n",
    "                             wd=0.0)\n",
    "            conv = tf.nn.conv2d(conv, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = self._variable_on_cpu('biases', [self.num_conv_2], tf.constant_initializer(0.0))\n",
    "            pre_activation = tf.nn.bias_add(conv, biases)\n",
    "            conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "            self._activation_summary(conv2)\n",
    "#         norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "        pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], [1, stride, stride, 1], padding='SAME', name='pool2')\n",
    "        conv = pool2\n",
    "        \n",
    "        if self.num_conv_3 > 0:\n",
    "            conv = self.conv_layer(3, pool2, self.num_conv_2, self.num_conv_3)\n",
    "        if self.num_conv_4 > 0:\n",
    "            conv = self.conv_layer(4, conv, self.num_conv_3, self.num_conv_4)\n",
    "        if self.num_conv_5 > 0:\n",
    "            conv = self.conv_layer(5, conv, self.num_conv_4, self.num_conv_5)\n",
    "        if self.num_conv_6 > 0:\n",
    "            conv = self.conv_layer(6, conv, self.num_conv_5, self.num_conv_6)\n",
    "        if self.num_conv_7 > 0:\n",
    "            conv = self.conv_layer(7, conv, self.num_conv_6, self.num_conv_7)\n",
    "        if self.num_conv_8 > 0:\n",
    "            conv = self.conv_layer(8, conv, self.num_conv_7, self.num_conv_8)\n",
    "        \n",
    "        with tf.variable_scope('local3') as scope:\n",
    "            reshape = tf.reshape(conv, [self.batch_size, -1])\n",
    "            dim = reshape.get_shape()[1].value\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[dim, self.num_fc_1],stddev=0.04, wd=0.004)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_fc_1], tf.constant_initializer(0.1))\n",
    "            local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "            self._activation_summary(local3)\n",
    "\n",
    "        with tf.variable_scope('local4') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[self.num_fc_1, self.num_fc_2],stddev=0.04, wd=0.004)\n",
    "            biases = self._variable_on_cpu('biases', [self.num_fc_2], tf.constant_initializer(0.1))\n",
    "            local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "            self._activation_summary(local4)\n",
    "\n",
    "        with tf.variable_scope('digit_length') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [self.num_fc_2, self.multi_digit_dataset.digit_count + 1],stddev=1.0/self.num_fc_2, wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [self.multi_digit_dataset.digit_count + 1],tf.constant_initializer(0.0))\n",
    "            digit_length_logit = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(digit_length_logit)\n",
    "\n",
    "        with tf.variable_scope('digits_0_logit') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [self.num_fc_2, 10],stddev=1.0/self.num_fc_2, wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [10],tf.constant_initializer(0.0))\n",
    "            digits_0_logit = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(digits_0_logit)\n",
    "        with tf.variable_scope('digits_1_logit') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [self.num_fc_2, 10],stddev=1.0/self.num_fc_2, wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [10],tf.constant_initializer(0.0))\n",
    "            digits_1_logit = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(digits_1_logit)\n",
    "        with tf.variable_scope('digits_2_logit') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [self.num_fc_2, 10],stddev=1.0/self.num_fc_2, wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [10],tf.constant_initializer(0.0))\n",
    "            digits_2_logit = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(digits_2_logit)\n",
    "        with tf.variable_scope('digits_3_logit') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [self.num_fc_2, 10],stddev=1.0/self.num_fc_2, wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [10],tf.constant_initializer(0.0))\n",
    "            digits_3_logit = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(digits_3_logit)\n",
    "        with tf.variable_scope('digits_4_logit') as scope:\n",
    "            weights = self._variable_with_weight_decay('weights', [self.num_fc_2, 10],stddev=1.0/self.num_fc_2, wd=0.0)\n",
    "            biases = self._variable_on_cpu('biases', [10],tf.constant_initializer(0.0))\n",
    "            digits_4_logit = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "            self._activation_summary(digits_4_logit)\n",
    "\n",
    "        return digit_length_logit, digits_0_logit, digits_1_logit, digits_2_logit, digits_3_logit, digits_4_logit\n",
    "\n",
    "    \n",
    "    def loss_(self, logits, length_label, digit_labels):\n",
    "        # Calculate the average cross entropy loss across the batch.\n",
    "        length_label = tf.cast(length_label, tf.int64)\n",
    "        digit_labels = tf.cast(digit_labels, tf.int64)\n",
    "#         print(logits[0], length_label, digit_labels)\n",
    "        cross_entropy_sum = tf.nn.softmax_cross_entropy_with_logits(logits[0], length_label, name='cross_entropy_for_length')\n",
    "        for i in range(0, self.digit_count):\n",
    "#             print(i, logits[i + 1], digit_labels[:,i])\n",
    "            cross_entropy_sum += tf.nn.softmax_cross_entropy_with_logits(logits[i + 1], digit_labels[:, i], name='cross_entropy_for_digit' + str(i))\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy_sum, name='cross_entropy')\n",
    "        tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "        # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "        # decay terms (L2 loss).\n",
    "        return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    \n",
    "    def train(self, total_loss, global_step):\n",
    "        # Variables that affect learning rate.\n",
    "        num_batches_per_epoch = self.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / self.batch_size\n",
    "        decay_steps = int(num_batches_per_epoch * self.NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "        # Decay the learning rate exponentially based on the number of steps.\n",
    "        lr = tf.train.exponential_decay(self.learning_rate_start,\n",
    "                                      global_step,\n",
    "                                      decay_steps,\n",
    "                                      self.learning_rate_decay_rate,\n",
    "                                      staircase=True)\n",
    "        tf.scalar_summary('learning_rate', lr)\n",
    "\n",
    "        # Generate moving averages of all losses and associated summaries.\n",
    "        loss_averages_op = self._add_loss_summaries(total_loss)\n",
    "\n",
    "        # Compute gradients.\n",
    "        with tf.control_dependencies([loss_averages_op]):\n",
    "            opt = tf.train.GradientDescentOptimizer(lr)\n",
    "            grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "        # Apply gradients.\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "        # Add histograms for trainable variables.\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.histogram_summary(var.op.name, var)\n",
    "\n",
    "        # Add histograms for gradients.\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.histogram_summary(var.op.name + '/gradients', grad)\n",
    "\n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(self.MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        import time\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # Input data.\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(\n",
    "                self.batch_size, self.multi_digit_dataset.image_width, self.multi_digit_dataset.image_height,\n",
    "                self.multi_digit_dataset.num_channels))\n",
    "            print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "            tf_train_length_labels = tf.placeholder(tf.float32,\n",
    "                                                    shape=(self.batch_size, self.multi_digit_dataset.digit_count + 1))\n",
    "            print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "            tf_train_digits_labels = tf.placeholder(tf.float32,\n",
    "                                                    shape=(self.batch_size, self.multi_digit_dataset.digit_count, 10))\n",
    "            print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "            tf_valid_dataset = tf.constant(self.multi_digit_dataset.validation_data)\n",
    "            tf_test_dataset = tf.constant(self.multi_digit_dataset.test_data)\n",
    "\n",
    "            # Training computation.\n",
    "            logits = self.inference(tf_train_dataset, True)\n",
    "            #   digits_1_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 1))\n",
    "            #   digits_2_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 2))\n",
    "            #   digits_3_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 3))\n",
    "            #   digits_4_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 4))\n",
    "            loss = self.loss_(logits, tf_train_length_labels, tf_train_digits_labels)\n",
    "            \n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            train_op = self.train(loss, global_step)\n",
    "\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            train_length_prediction = tf.nn.softmax(logits[0])\n",
    "            train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "            train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "            train_digits_2_prediction = tf.nn.softmax(logits[3])\n",
    "            train_digits_3_prediction = tf.nn.softmax(logits[4])\n",
    "            train_digits_4_prediction = tf.nn.softmax(logits[5])\n",
    "\n",
    "            summary_op = tf.merge_all_summaries()\n",
    "            init = tf.initialize_all_variables()\n",
    "            session = tf.Session()\n",
    "            session.run(init)\n",
    "            summary_writer = tf.train.SummaryWriter('logs/tensorboard/voyageth/svhn/' + time.strftime(\"%Y-%m-%dT%H:%M:%S%z\"), session.graph)\n",
    "\n",
    "            print('Initialized')\n",
    "            for step in range(self.num_steps):\n",
    "                offset = (step * self.batch_size) % (self.multi_digit_dataset.train_data.shape[0] - self.batch_size)\n",
    "                batch_data = self.multi_digit_dataset.train_data[offset:(offset + self.batch_size), :, :, :]\n",
    "                batch_length_labels = self.multi_digit_dataset.train_label_length[offset:(offset + self.batch_size), 0,\n",
    "                                      :]\n",
    "                batch_digits_labels = self.multi_digit_dataset.train_label_digits[offset:(offset + self.batch_size), :]\n",
    "\n",
    "                feed_dict = {tf_train_dataset: batch_data, tf_train_length_labels: batch_length_labels,\n",
    "                             tf_train_digits_labels: batch_digits_labels}\n",
    "                \n",
    "                _, summary_str, l, p_length, p_0, p_1, p_2, p_3, p_4 = session.run([train_op, summary_op, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction,\n",
    "                     train_digits_2_prediction, train_digits_3_prediction, train_digits_4_prediction],feed_dict=feed_dict)\n",
    "                if step % 100 == 0:\n",
    "                    summary_writer.add_summary(summary_str, step)\n",
    "                \n",
    "                if step % 50 == 0:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    accuracy_result = self.accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels,\n",
    "                                                           batch_digits_labels)\n",
    "                    print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "                    for k in range(0, self.digit_count):\n",
    "                        accuracy_result = self.accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels,\n",
    "                                                              batch_digits_labels, k)\n",
    "                        print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n",
    "                    accuracy_result = self.accuracy(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels,\n",
    "                                                    batch_digits_labels)\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy_result)\n",
    "                    print(\"finish : {}\".format(time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")))\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svhn/svhn_2_28_100000.pickle already present - Skipping pickling.\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(64, 28, 28, 3), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(64, 3), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(64, 2, 10), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 6.440044\n",
      "Minibatch accuracy_length: 0.0%\n",
      "Minibatch accuracy_digit_0: 3.1%\n",
      "Minibatch accuracy_digit_1: 9.4%\n",
      "Minibatch accuracy: 0.0%\n",
      "finish : 2016-11-21T08:53:28+0900\n",
      "Minibatch loss at step 50: 5.074582\n",
      "Minibatch accuracy_length: 81.2%\n",
      "Minibatch accuracy_digit_0: 26.6%\n",
      "Minibatch accuracy_digit_1: 6.2%\n",
      "Minibatch accuracy: 0.0%\n",
      "finish : 2016-11-21T08:53:39+0900\n",
      "Minibatch loss at step 100: 4.857594\n",
      "Minibatch accuracy_length: 64.1%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch accuracy_digit_1: 15.6%\n",
      "Minibatch accuracy: 1.6%\n",
      "finish : 2016-11-21T08:53:50+0900\n",
      "Minibatch loss at step 150: 7.352870\n",
      "Minibatch accuracy_length: 75.0%\n",
      "Minibatch accuracy_digit_0: 18.8%\n",
      "Minibatch accuracy_digit_1: 10.9%\n",
      "Minibatch accuracy: 0.0%\n",
      "finish : 2016-11-21T08:54:01+0900\n",
      "Minibatch loss at step 200: 5.042676\n",
      "Minibatch accuracy_length: 73.4%\n",
      "Minibatch accuracy_digit_0: 14.1%\n",
      "Minibatch accuracy_digit_1: 7.8%\n",
      "Minibatch accuracy: 0.0%\n",
      "finish : 2016-11-21T08:54:12+0900\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Infinity in summary histogram for: local4/HistogramSummary\n\t [[Node: local4/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/HistogramSummary/tag, local4/local4)]]\n\nCaused by op u'local4/HistogramSummary', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-1e1bea26fb77>\", line 9, in <module>\n    num_fc_2=64,\n  File \"<ipython-input-12-fb76bd5bd76b>\", line 341, in run\n    logits = self.inference(tf_train_dataset, True)\n  File \"<ipython-input-12-fb76bd5bd76b>\", line 225, in inference\n    self._activation_summary(local4)\n  File \"<ipython-input-12-fb76bd5bd76b>\", line 115, in _activation_summary\n    tf.histogram_summary(x.op.name + '/activations', x)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/logging_ops.py\", line 100, in histogram_summary\n    tag=tag, values=values, name=scope)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 100, in _histogram_summary\n    name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Infinity in summary histogram for: local4/HistogramSummary\n\t [[Node: local4/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/HistogramSummary/tag, local4/local4)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1e1bea26fb77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mnum_convs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mnum_fc_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         \u001b[0mnum_fc_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                        ).run()\n",
      "\u001b[0;32m<ipython-input-12-fb76bd5bd76b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 _, summary_str, l, p_length, p_0, p_1, p_2, p_3, p_4 = session.run([train_op, summary_op, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction,\n\u001b[0;32m--> 377\u001b[0;31m                      train_digits_2_prediction, train_digits_3_prediction, train_digits_4_prediction],feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Infinity in summary histogram for: local4/HistogramSummary\n\t [[Node: local4/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/HistogramSummary/tag, local4/local4)]]\n\nCaused by op u'local4/HistogramSummary', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-1e1bea26fb77>\", line 9, in <module>\n    num_fc_2=64,\n  File \"<ipython-input-12-fb76bd5bd76b>\", line 341, in run\n    logits = self.inference(tf_train_dataset, True)\n  File \"<ipython-input-12-fb76bd5bd76b>\", line 225, in inference\n    self._activation_summary(local4)\n  File \"<ipython-input-12-fb76bd5bd76b>\", line 115, in _activation_summary\n    tf.histogram_summary(x.op.name + '/activations', x)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/logging_ops.py\", line 100, in histogram_summary\n    tag=tag, values=values, name=scope)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 100, in _histogram_summary\n    name=name)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Infinity in summary histogram for: local4/HistogramSummary\n\t [[Node: local4/HistogramSummary = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/HistogramSummary/tag, local4/local4)]]\n"
     ]
    }
   ],
   "source": [
    "svhn_data = MultiDigitSVHNData.maybe_pickle(digit_count=2, total_data_count=100000, image_size=28)\n",
    "MultiDigitModel(multi_digit_dataset=svhn_data,\n",
    "                        digit_count=svhn_data.digit_count,\n",
    "                        num_channels=3,\n",
    "                        num_steps=10001, \n",
    "                        batch_size=64, \n",
    "                        num_convs=[16,32,64,0,0,0,0,0], \n",
    "                        num_fc_1=64, \n",
    "                        num_fc_2=64,\n",
    "                       ).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
