{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/document/d/1L11EjK0uObqjaBhHNcVPxeyIripGHSUaoEWGypuuVtk/pub\n",
    "\n",
    "# Description\n",
    "\n",
    "Objective: Build a live camera app that can interpret number strings in real-world images.\n",
    "\n",
    "In this project, you will train a model that can decode sequences of digits from natural images, and create an app that prints the numbers it sees in real time. You may choose to implement your project as a simple Python script, a web app/service or an Android app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Design and test a model architecture that can identify sequences of digits in an image.\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train it using synthetic data first (recommended) or directly use real-world data (see step 2).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "To help you develop your model, the simplest path is likely to generate a synthetic dataset by concatenating character images from [notMNIST](https://www.google.com/url?q=http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html&sa=D&ust=1476236462755000&usg=AFQjCNHAxfxTiYixUmTSqZNPuGhfXBg6Cg) or [MNIST](https://www.google.com/url?q=http://yann.lecun.com/exdb/mnist/&sa=D&ust=1476236462756000&usg=AFQjCNFiIq8Kn5_Lm-p8qu_L67mAttbsfg). This can provide you with a quick way to run experiments. (Or you can go directly to the real-world dataset of Step 2.)\n",
    "\n",
    "In order to produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "Here is for example [a published baseline model](https://www.google.com/url?q=http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf&sa=D&ust=1476236462758000&usg=AFQjCNFYKwUmWGu1HE0PYqPHYt5l4N66pw) on this problem ([video](https://www.google.com/url?q=https://www.youtube.com/watch?v%3DvGPI_JvLoN0&sa=D&ust=1476236462759000&usg=AFQjCNFVU9fIkilx9J_FKlBBuYRZ9CGWoQ)).\n",
    "\n",
    "- _What approach did you take in coming up with a solution to this problem?_\n",
    "- _What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_\n",
    "- _How did you train your model? Did you generate a synthetic dataset (if so, explain how)?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build baseline model http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf\n",
    "# Multi-digit Number Recognition\n",
    "# Deep Convolutional Neural Networks\n",
    "\n",
    "# In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels.\n",
    "# We employ the DistBelief (Dean et al., 2012) implementation of deep neural networks\n",
    "# We find that the performance of this approach increases with the depth of the convolutional network,\n",
    "# with the best performance occurring in the deepest architecture we trained, with eleven hidden layers.\n",
    "# We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers\n",
    "# We show that on a per-digit recognition task, we improve upon the state-of-the-art and achieve 97.84% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiDigitMNISTData(object):\n",
    "    def __init__(self, digit_count=1):\n",
    "        from sklearn.datasets import fetch_mldata\n",
    "        mnist = fetch_mldata('MNIST original')\n",
    "        \n",
    "        self.mnist = mnist\n",
    "        self.digit_count = digit_count\n",
    "        self.image_width = 28\n",
    "        self.image_height = 28 * self.digit_count\n",
    "        self.target_digit_num_labels = 10\n",
    "        self.num_channels = 1\n",
    "        \n",
    "        self.total_data_count = len(mnist.data)\n",
    "        self.train_data_count = int(0.7 * self.total_data_count)\n",
    "        self.validation_data_count = int(0.2 * self.total_data_count)\n",
    "        self.test_data_count = self.total_data_count - self.train_data_count - self.validation_data_count\n",
    "        \n",
    "        self.multi_digit_mnist_data = np.zeros([self.total_data_count, self.image_width * self.image_height],\n",
    "                                               dtype=np.float32)\n",
    "        self.multi_digit_mnist_target_length = np.zeros([self.total_data_count, 1],\n",
    "                                                        dtype=np.float32)\n",
    "        self.multi_digit_mnist_target_digits = np.zeros([self.total_data_count, self.digit_count],\n",
    "                                                        dtype=np.float32)\n",
    "        \n",
    "        self.train_data = None\n",
    "        self.train_label_length = None\n",
    "        self.train_label_digits = None\n",
    "        self.validation_data = None\n",
    "        self.validation_label_length = None\n",
    "        self.validation_label_digits = None\n",
    "        self.test_data = None\n",
    "        self.test_label_length = None\n",
    "        self.test_label_digits = None\n",
    "        \n",
    "        \n",
    "    def reformat_dataset(self, dataset):\n",
    "        return dataset.reshape((-1, self.image_width, self.image_height, self.num_channels)).astype(np.float32)\n",
    "    \n",
    "    def reformat_target_length(self, target_length):\n",
    "        return (np.arange(self.digit_count + 1) == target_length[:,None]).astype(np.float32)\n",
    "    \n",
    "    def reformat_target_digits(self, target_digits):\n",
    "        return (np.arange(self.target_digit_num_labels) == target_digits[:,:,None]).astype(np.float32)\n",
    "    \n",
    "    def load_data(self):\n",
    "        import random\n",
    "        for i in range(0, self.total_data_count):\n",
    "            random_length = random.randrange(1,self.digit_count+1)\n",
    "            self.multi_digit_mnist_target_length[i, 0] = random_length\n",
    "            for j in range(0, random_length):\n",
    "                random_index = random.randrange(0, self.total_data_count)\n",
    "#                 print(\"j={}, j*784={}\".format(j, j*784))\n",
    "#                 print(\"self.multi_digit_mnist_data[i, j * 784:(j+1) * 784].shape : {}\".format(self.multi_digit_mnist_data[i, j * 784:(j+1) * 784].shape))\n",
    "#                 print(\"self.mnist.data[random_index].shape : {}\".format(self.mnist.data[random_index].shape))\n",
    "                self.multi_digit_mnist_data[i, j * 784:(j+1) * 784] = self.mnist.data[random_index] / 255.0\n",
    "                self.multi_digit_mnist_target_digits[i, j] = self.mnist.target[random_index]\n",
    "        print(self.multi_digit_mnist_data.shape) # (70000, 3920)\n",
    "        print(self.multi_digit_mnist_target_length.shape) # (70000, 1)\n",
    "        print(self.multi_digit_mnist_target_digits.shape) # (70000, 5)\n",
    "        \n",
    "        self.train_data = self.reformat_dataset(self.multi_digit_mnist_data[:self.train_data_count])\n",
    "        self.train_label_length = self.reformat_target_length(self.multi_digit_mnist_target_length[:self.train_data_count])\n",
    "        self.train_label_digits = self.reformat_target_digits(self.multi_digit_mnist_target_digits[:self.train_data_count])\n",
    "        print(self.train_data.shape) # (49000, 140, 28, 1)\n",
    "        print(self.train_label_length.shape) # (49000, 1, 6)\n",
    "        print(self.train_label_digits.shape) # (49000, 5, 10)\n",
    "        \n",
    "        self.validation_data = self.reformat_dataset(self.multi_digit_mnist_data[self.train_data_count:self.train_data_count + self.validation_data_count])\n",
    "        self.validation_label_length = self.reformat_target_length(self.multi_digit_mnist_target_length[self.train_data_count:self.train_data_count + self.validation_data_count])\n",
    "        self.validation_label_digits = self.reformat_target_digits(self.multi_digit_mnist_target_digits[self.train_data_count:self.train_data_count + self.validation_data_count])\n",
    "        print(self.validation_data.shape) # (14000, 140, 28, 1)\n",
    "        print(self.validation_label_length.shape) # (14000, 1, 6)\n",
    "        print(self.validation_label_digits.shape) # (14000, 5, 10)\n",
    "        \n",
    "        self.test_data = self.reformat_dataset(self.multi_digit_mnist_data[self.train_data_count + self.validation_data_count:])\n",
    "        self.test_label_length = self.reformat_target_length(self.multi_digit_mnist_target_length[self.train_data_count + self.validation_data_count:])\n",
    "        self.test_label_digits = self.reformat_target_digits(self.multi_digit_mnist_target_digits[self.train_data_count + self.validation_data_count:])\n",
    "        print(self.test_data.shape) # (7000, 140, 28, 1)\n",
    "        print(self.test_label_length.shape) # (7000, 1, 6)\n",
    "        print(self.test_label_digits.shape) # (7000, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (100, 6) vs (100, 6)\n",
    "# logits[1] : Tensor(\"add_4:0\", shape=(16, 10), dtype=float32)\n",
    "# tf_train_digits_labels[:,0] : Tensor(\"strided_slice_5:0\", shape=(16, 10), dtype=float32)\n",
    "# train_length_prediction : Tensor(\"Softmax:0\", shape=(16, 6), dtype=float32)\n",
    "# train_digits_0_prediction : Tensor(\"Softmax_1:0\", shape=(16, 10), dtype=float32)\n",
    "# p_length, p_0, p_1, p_2, p_3, p_4, \n",
    "# train_length_prediction, train_digits_0_prediction, train_digits_1_prediction, train_digits_2_prediction, train_digits_3_prediction, train_digits_4_prediction\n",
    "def accuracy_digit(p_length, p_digits, \n",
    "             batch_length_labels, batch_digits_labels, digit):\n",
    "  eq_count = 0.0\n",
    "  total_count = 0.0\n",
    "  for i in range(0, len(p_digits[digit])):\n",
    "#     print(\"length:{}\".format(np.argmax(batch_length_labels[i])))\n",
    "    if np.argmax(batch_length_labels[i]) > digit:\n",
    "        total_count += 1.0\n",
    "        if np.argmax(p_digits[digit][i]) == np.argmax(batch_digits_labels[i][digit]):\n",
    "          eq_count += 1.0\n",
    "#           print(\"Correct Predict. predict:{}, real:{}\".format(np.argmax(p_digits[digit][i]), np.argmax(batch_digits_labels[i][digit])))\n",
    "#         else:\n",
    "#           print(\"False Predict. predict:{}, real:{}\".format(np.argmax(p_digits[digit][i]), np.argmax(batch_digits_labels[i][digit])))\n",
    "  return eq_count / total_count * 100\n",
    "\n",
    "def accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, \n",
    "             batch_length_labels, batch_digits_labels):\n",
    "  eq_count = 0.0\n",
    "  for i in range(0, len(p_0)):\n",
    "    if np.argmax(p_length[i]) == np.argmax(batch_length_labels[i]):\n",
    "      eq_count += 1.0\n",
    "  return eq_count / len(p_0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 한개 숫자 이미지\n",
    "- 예측 : 한개 숫자 + 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1)\n",
      "(70000, 1)\n",
      "(49000, 28, 28, 1)\n",
      "(49000, 1, 2)\n",
      "(49000, 1, 10)\n",
      "(14000, 28, 28, 1)\n",
      "(14000, 1, 2)\n",
      "(14000, 1, 10)\n",
      "(7000, 28, 28, 1)\n",
      "(7000, 1, 2)\n",
      "(7000, 1, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(16, 28, 28, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(16, 2), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(16, 1, 10), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 7.147527\n",
      "Minibatch accuracy_length: 0.0%\n",
      "Minibatch accuracy_digit_0: 6.2%\n",
      "Minibatch loss at step 50: 2.337904\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 6.2%\n",
      "Minibatch loss at step 100: 2.258426\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 18.8%\n",
      "Minibatch loss at step 150: 2.255040\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch loss at step 200: 1.599000\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 50.0%\n",
      "Minibatch loss at step 250: 0.797102\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 300: 0.812372\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch loss at step 350: 0.970195\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch loss at step 400: 1.014020\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch loss at step 450: 0.340962\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 500: 1.100860\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch loss at step 550: 1.121633\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch loss at step 600: 0.389291\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 650: 0.449291\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 700: 0.099418\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 100.0%\n",
      "Minibatch loss at step 750: 0.639497\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 800: 0.434785\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 850: 0.173318\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 900: 0.399974\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 950: 0.069075\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 100.0%\n",
      "Minibatch loss at step 1000: 0.268006\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "depth_1 = depth\n",
    "depth_2 = depth\n",
    "depth_3 = depth\n",
    "num_hidden = 64\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(1)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    conv = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(conv, digit_length_weights) + digit_length_biases\n",
    "    digits_0_logit = tf.matmul(conv, digits_0_weights) + digits_0_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,1):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 두개 숫자 이미지\n",
    "- 예측 : 한개 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1568)\n",
      "(70000, 1)\n",
      "(70000, 2)\n",
      "(49000, 28, 56, 1)\n",
      "(49000, 1, 3)\n",
      "(49000, 2, 10)\n",
      "(14000, 28, 56, 1)\n",
      "(14000, 1, 3)\n",
      "(14000, 2, 10)\n",
      "(7000, 28, 56, 1)\n",
      "(7000, 1, 3)\n",
      "(7000, 2, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(16, 28, 56, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(16, 3), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(16, 2, 10), dtype=float32)\n",
      "layer2 Tensor(\"Relu_1:0\", shape=(16, 7, 14, 16), dtype=float32) \n",
      "Initialized\n",
      "Minibatch loss at step 0: 4.093553\n",
      "Minibatch accuracy_length: 0.0%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch loss at step 50: 2.368903\n",
      "Minibatch accuracy_length: 0.0%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch loss at step 100: 2.254499\n",
      "Minibatch accuracy_length: 0.0%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch loss at step 150: 2.006486\n",
      "Minibatch accuracy_length: 37.5%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch loss at step 200: 1.631744\n",
      "Minibatch accuracy_length: 62.5%\n",
      "Minibatch accuracy_digit_0: 50.0%\n",
      "Minibatch loss at step 250: 1.230651\n",
      "Minibatch accuracy_length: 12.5%\n",
      "Minibatch accuracy_digit_0: 37.5%\n",
      "Minibatch loss at step 300: 0.984956\n",
      "Minibatch accuracy_length: 25.0%\n",
      "Minibatch accuracy_digit_0: 56.2%\n",
      "Minibatch loss at step 350: 0.560260\n",
      "Minibatch accuracy_length: 12.5%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch loss at step 400: 0.780425\n",
      "Minibatch accuracy_length: 43.8%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 450: 0.389828\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 500: 0.434910\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 550: 0.379687\n",
      "Minibatch accuracy_length: 25.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 600: 0.397143\n",
      "Minibatch accuracy_length: 25.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 650: 0.281625\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 700: 0.356269\n",
      "Minibatch accuracy_length: 37.5%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 750: 0.100289\n",
      "Minibatch accuracy_length: 37.5%\n",
      "Minibatch accuracy_digit_0: 100.0%\n",
      "Minibatch loss at step 800: 1.172704\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch loss at step 850: 0.458186\n",
      "Minibatch accuracy_length: 12.5%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 900: 0.171129\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch loss at step 950: 0.427940\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 1000: 0.645294\n",
      "Minibatch accuracy_length: 31.2%\n",
      "Minibatch accuracy_digit_0: 93.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "depth_1 = depth\n",
    "depth_2 = depth\n",
    "depth_3 = depth\n",
    "depth_4 = depth\n",
    "num_hidden = 64\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(2)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_4)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, stride, stride, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    print(\"layer2 {} \".format(conv))\n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "#     print(\"layer3 {} \".format(conv))\n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    conv = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(conv, digit_length_weights) + digit_length_biases\n",
    "    digits_0_logit = tf.matmul(conv, digits_0_weights) + digits_0_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "#         tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,1):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 두개 숫자\n",
    "- 예측 : 한개 숫자 + 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1568)\n",
      "(70000, 1)\n",
      "(70000, 2)\n",
      "(49000, 28, 56, 1)\n",
      "(49000, 1, 3)\n",
      "(49000, 2, 10)\n",
      "(14000, 28, 56, 1)\n",
      "(14000, 1, 3)\n",
      "(14000, 2, 10)\n",
      "(7000, 28, 56, 1)\n",
      "(7000, 1, 3)\n",
      "(7000, 2, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(16, 28, 56, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(16, 3), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(16, 2, 10), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(16, 14, 28, 8), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(16, 7, 14, 16), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 10.225079\n",
      "Minibatch accuracy_length: 12.5%\n",
      "Minibatch accuracy_digit_0: 6.2%\n",
      "Minibatch loss at step 50: 2.892925\n",
      "Minibatch accuracy_length: 50.0%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch loss at step 100: 3.013327\n",
      "Minibatch accuracy_length: 62.5%\n",
      "Minibatch accuracy_digit_0: 0.0%\n",
      "Minibatch loss at step 150: 2.834291\n",
      "Minibatch accuracy_length: 75.0%\n",
      "Minibatch accuracy_digit_0: 0.0%\n",
      "Minibatch loss at step 200: 2.237595\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch loss at step 250: 2.023841\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch loss at step 300: 1.687297\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 31.2%\n",
      "Minibatch loss at step 350: 1.322602\n",
      "Minibatch accuracy_length: 93.8%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch loss at step 400: 1.577786\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 50.0%\n",
      "Minibatch loss at step 450: 1.140655\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 56.2%\n",
      "Minibatch loss at step 500: 1.977182\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 43.8%\n",
      "Minibatch loss at step 550: 0.967900\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch loss at step 600: 0.781384\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 650: 1.073429\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch loss at step 700: 0.962227\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch loss at step 750: 0.734425\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch loss at step 800: 0.557871\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 850: 0.393643\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 900: 0.549438\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 950: 0.557016\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch loss at step 1000: 0.542713\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 8\n",
    "depth_1 = depth\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 2\n",
    "depth_4 = depth_3 * 2\n",
    "num_hidden = 32\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(2)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     reshape = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    reshape = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(reshape, digit_length_weights) + digit_length_biases\n",
    "    digits_0_logit = tf.matmul(reshape, digits_0_weights) + digits_0_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,1):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 두개 숫자\n",
    "- 출력 : 두개 숫자 + 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1568)\n",
      "(70000, 1)\n",
      "(70000, 2)\n",
      "(49000, 28, 56, 1)\n",
      "(49000, 1, 3)\n",
      "(49000, 2, 10)\n",
      "(14000, 28, 56, 1)\n",
      "(14000, 1, 3)\n",
      "(14000, 2, 10)\n",
      "(7000, 28, 56, 1)\n",
      "(7000, 1, 3)\n",
      "(7000, 2, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(32, 28, 56, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(32, 3), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(32, 2, 10), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(32, 14, 28, 8), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(32, 7, 14, 16), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 9.903417\n",
      "Minibatch accuracy_length: 40.6%\n",
      "Minibatch accuracy_digit_0: 21.9%\n",
      "Minibatch accuracy_digit_1: 7.7%\n",
      "Minibatch loss at step 100: 3.579196\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 9.4%\n",
      "Minibatch accuracy_digit_1: 31.2%\n",
      "Minibatch loss at step 200: 3.169344\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 31.2%\n",
      "Minibatch accuracy_digit_1: 47.1%\n",
      "Minibatch loss at step 300: 3.135514\n",
      "Minibatch accuracy_length: 93.8%\n",
      "Minibatch accuracy_digit_0: 37.5%\n",
      "Minibatch accuracy_digit_1: 50.0%\n",
      "Minibatch loss at step 400: 2.178575\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 56.2%\n",
      "Minibatch accuracy_digit_1: 61.5%\n",
      "Minibatch loss at step 500: 1.915064\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 57.1%\n",
      "Minibatch loss at step 600: 1.659961\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 80.0%\n",
      "Minibatch loss at step 700: 1.560274\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 50.0%\n",
      "Minibatch loss at step 800: 1.799039\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 50.0%\n",
      "Minibatch loss at step 900: 0.969051\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 77.8%\n",
      "Minibatch loss at step 1000: 1.159756\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 88.9%\n",
      "Minibatch loss at step 1100: 0.930255\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 70.6%\n",
      "Minibatch loss at step 1200: 1.490840\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 68.4%\n",
      "Minibatch loss at step 1300: 1.073874\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 73.3%\n",
      "Minibatch loss at step 1400: 0.512806\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 100.0%\n",
      "Minibatch loss at step 1500: 0.853845\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 86.7%\n",
      "Minibatch loss at step 1600: 0.955724\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 76.9%\n",
      "Minibatch loss at step 1700: 0.664105\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 80.0%\n",
      "Minibatch loss at step 1800: 1.050077\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 93.8%\n",
      "Minibatch loss at step 1900: 0.588806\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 87.5%\n",
      "Minibatch loss at step 2000: 0.616197\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 92.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 8\n",
    "depth_1 = depth\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 2\n",
    "depth_4 = depth_3 * 2\n",
    "num_hidden = 32\n",
    "\n",
    "pooling_stride = 2\n",
    "pooling_kernel_size = 2\n",
    "beta = 0.001\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(2)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "#     print(conv)\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     reshape = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    reshape = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(reshape, digit_length_weights) + digit_length_biases    \n",
    "    digit_length_logit_inner = tf.to_float(tf.argmax(tf.nn.softmax(digit_length_logit), dimension=1))\n",
    "    \n",
    "    digits_0_logit = tf.matmul(reshape, digits_0_weights) + digits_0_biases\n",
    "    digits_1_logit = tf.matmul(reshape, digits_1_weights) + digits_1_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit, digits_1_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  digits_1_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 1))\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "        + digits_1_mult* tf.nn.softmax_cross_entropy_with_logits(logits[2], tf_train_digits_labels[:,1])\n",
    "        + beta * tf.nn.l2_loss(layer1_weights) \n",
    "        + beta * tf.nn.l2_loss(layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(fc_layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(digit_length_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_0_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_1_weights) \n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "  train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0, p_1 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,2):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 숫자 3개\n",
    "- 출력 : 길이 + 숫자 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2352)\n",
      "(70000, 1)\n",
      "(70000, 3)\n",
      "(49000, 28, 84, 1)\n",
      "(49000, 1, 4)\n",
      "(49000, 3, 10)\n",
      "(14000, 28, 84, 1)\n",
      "(14000, 1, 4)\n",
      "(14000, 3, 10)\n",
      "(7000, 28, 84, 1)\n",
      "(7000, 1, 4)\n",
      "(7000, 3, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(32, 28, 84, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(32, 4), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(32, 3, 10), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(32, 14, 42, 8), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(32, 7, 21, 16), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 16.063213\n",
      "Minibatch accuracy_length: 28.1%\n",
      "Minibatch accuracy_digit_0: 0.0%\n",
      "Minibatch accuracy_digit_1: 10.5%\n",
      "Minibatch loss at step 100: 4.341612\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch accuracy_digit_1: 19.0%\n",
      "Minibatch loss at step 200: 3.618831\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 37.5%\n",
      "Minibatch accuracy_digit_1: 21.7%\n",
      "Minibatch loss at step 300: 3.101359\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 40.6%\n",
      "Minibatch accuracy_digit_1: 36.4%\n",
      "Minibatch loss at step 400: 2.742776\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 65.6%\n",
      "Minibatch accuracy_digit_1: 29.6%\n",
      "Minibatch loss at step 500: 2.891159\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 46.9%\n",
      "Minibatch accuracy_digit_1: 30.0%\n",
      "Minibatch loss at step 600: 2.167923\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 40.9%\n",
      "Minibatch loss at step 700: 2.464239\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 45.5%\n",
      "Minibatch loss at step 800: 2.315191\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 56.2%\n",
      "Minibatch accuracy_digit_1: 61.9%\n",
      "Minibatch loss at step 900: 1.944423\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 47.6%\n",
      "Minibatch loss at step 1000: 1.759259\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 76.2%\n",
      "Minibatch loss at step 1100: 1.362163\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 66.7%\n",
      "Minibatch loss at step 1200: 1.658338\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 61.9%\n",
      "Minibatch loss at step 1300: 0.931365\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 93.3%\n",
      "Minibatch loss at step 1400: 0.996684\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 94.7%\n",
      "Minibatch loss at step 1500: 1.121360\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 71.4%\n",
      "Minibatch loss at step 1600: 1.187496\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 69.2%\n",
      "Minibatch loss at step 1700: 1.234050\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 80.0%\n",
      "Minibatch loss at step 1800: 1.285247\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 76.2%\n",
      "Minibatch loss at step 1900: 0.811461\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 85.0%\n",
      "Minibatch loss at step 2000: 0.764505\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 91.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 8\n",
    "depth_1 = depth\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 2\n",
    "depth_4 = depth_3 * 2\n",
    "num_hidden = 32\n",
    "\n",
    "pooling_stride = 2\n",
    "pooling_kernel_size = 2\n",
    "beta = 0.001\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(3)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "#     print(conv)\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     reshape = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    reshape = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(reshape, digit_length_weights) + digit_length_biases    \n",
    "    digit_length_logit_inner = tf.to_float(tf.argmax(tf.nn.softmax(digit_length_logit), dimension=1))\n",
    "    \n",
    "    digits_0_logit = tf.matmul(reshape, digits_0_weights) + digits_0_biases\n",
    "    digits_1_logit = tf.matmul(reshape, digits_1_weights) + digits_1_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit, digits_1_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  digits_1_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 1))\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "        + digits_1_mult* tf.nn.softmax_cross_entropy_with_logits(logits[2], tf_train_digits_labels[:,1])\n",
    "        + beta * tf.nn.l2_loss(layer1_weights) \n",
    "        + beta * tf.nn.l2_loss(layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(fc_layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(digit_length_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_0_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_1_weights) \n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "  train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0, p_1 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,2):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 숫자 3개\n",
    "- 출력 : 숫자 3개 + 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2352)\n",
      "(70000, 1)\n",
      "(70000, 3)\n",
      "(49000, 28, 84, 1)\n",
      "(49000, 1, 4)\n",
      "(49000, 3, 10)\n",
      "(14000, 28, 84, 1)\n",
      "(14000, 1, 4)\n",
      "(14000, 3, 10)\n",
      "(7000, 28, 84, 1)\n",
      "(7000, 1, 4)\n",
      "(7000, 3, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(32, 28, 84, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(32, 4), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(32, 3, 10), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(32, 14, 42, 8), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(32, 7, 21, 16), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 11.962111\n",
      "Minibatch accuracy_length: 37.5%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch accuracy_digit_1: 13.0%\n",
      "Minibatch accuracy_digit_2: 0.0%\n",
      "Minibatch loss at step 100: 5.999780\n",
      "Minibatch accuracy_length: 43.8%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch accuracy_digit_1: 11.1%\n",
      "Minibatch accuracy_digit_2: 7.7%\n",
      "Minibatch loss at step 200: 4.949679\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 9.4%\n",
      "Minibatch accuracy_digit_1: 0.0%\n",
      "Minibatch accuracy_digit_2: 10.0%\n",
      "Minibatch loss at step 300: 5.518234\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 21.9%\n",
      "Minibatch accuracy_digit_1: 14.3%\n",
      "Minibatch accuracy_digit_2: 21.4%\n",
      "Minibatch loss at step 400: 4.274510\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 40.6%\n",
      "Minibatch accuracy_digit_1: 18.2%\n",
      "Minibatch accuracy_digit_2: 20.0%\n",
      "Minibatch loss at step 500: 4.534950\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch accuracy_digit_1: 26.9%\n",
      "Minibatch accuracy_digit_2: 36.4%\n",
      "Minibatch loss at step 600: 4.185916\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 31.2%\n",
      "Minibatch accuracy_digit_1: 26.9%\n",
      "Minibatch accuracy_digit_2: 16.7%\n",
      "Minibatch loss at step 700: 3.118014\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 43.8%\n",
      "Minibatch accuracy_digit_1: 43.5%\n",
      "Minibatch accuracy_digit_2: 57.1%\n",
      "Minibatch loss at step 800: 3.326066\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch accuracy_digit_1: 57.1%\n",
      "Minibatch accuracy_digit_2: 30.8%\n",
      "Minibatch loss at step 900: 2.033848\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 50.0%\n",
      "Minibatch accuracy_digit_2: 25.0%\n",
      "Minibatch loss at step 1000: 3.155626\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 50.0%\n",
      "Minibatch accuracy_digit_1: 57.1%\n",
      "Minibatch accuracy_digit_2: 0.0%\n",
      "Minibatch loss at step 1100: 1.922413\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch accuracy_digit_1: 89.5%\n",
      "Minibatch accuracy_digit_2: 33.3%\n",
      "Minibatch loss at step 1200: 2.242657\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 60.0%\n",
      "Minibatch accuracy_digit_2: 54.5%\n",
      "Minibatch loss at step 1300: 1.752951\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 63.2%\n",
      "Minibatch accuracy_digit_2: 80.0%\n",
      "Minibatch loss at step 1400: 1.948511\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 76.0%\n",
      "Minibatch accuracy_digit_2: 62.5%\n",
      "Minibatch loss at step 1500: 1.559011\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 81.5%\n",
      "Minibatch accuracy_digit_2: 72.7%\n",
      "Minibatch loss at step 1600: 1.272259\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 84.2%\n",
      "Minibatch accuracy_digit_2: 80.0%\n",
      "Minibatch loss at step 1700: 1.256335\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 95.5%\n",
      "Minibatch accuracy_digit_2: 81.8%\n",
      "Minibatch loss at step 1800: 2.426616\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 65.6%\n",
      "Minibatch accuracy_digit_1: 83.3%\n",
      "Minibatch accuracy_digit_2: 58.3%\n",
      "Minibatch loss at step 1900: 1.691783\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 72.7%\n",
      "Minibatch accuracy_digit_2: 69.2%\n",
      "Minibatch loss at step 2000: 1.309577\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 90.9%\n",
      "Minibatch accuracy_digit_2: 80.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 8\n",
    "depth_1 = depth\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 2\n",
    "depth_4 = depth_3 * 2\n",
    "num_hidden = 32\n",
    "\n",
    "pooling_stride = 2\n",
    "pooling_kernel_size = 2\n",
    "beta = 0.001\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(3)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "#     print(conv)\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     reshape = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    reshape = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(reshape, digit_length_weights) + digit_length_biases    \n",
    "    digit_length_logit_inner = tf.to_float(tf.argmax(tf.nn.softmax(digit_length_logit), dimension=1))\n",
    "    \n",
    "    digits_0_logit = tf.matmul(reshape, digits_0_weights) + digits_0_biases\n",
    "    digits_1_logit = tf.matmul(reshape, digits_1_weights) + digits_1_biases\n",
    "    digits_2_logit = tf.matmul(reshape, digits_2_weights) + digits_2_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit, digits_1_logit, digits_2_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  digits_1_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 1))\n",
    "  digits_2_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 2))\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "        + digits_1_mult* tf.nn.softmax_cross_entropy_with_logits(logits[2], tf_train_digits_labels[:,1])\n",
    "        + digits_2_mult* tf.nn.softmax_cross_entropy_with_logits(logits[3], tf_train_digits_labels[:,2])\n",
    "        + beta * tf.nn.l2_loss(layer1_weights) \n",
    "        + beta * tf.nn.l2_loss(layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(fc_layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(digit_length_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_0_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_1_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_2_weights) \n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "  train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "  train_digits_2_prediction = tf.nn.softmax(logits[3])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0, p_1, p_2 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction, train_digits_2_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,3):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 숫자 4개\n",
    "- 출력 : 숫자 3개 + 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 3136)\n",
      "(70000, 1)\n",
      "(70000, 4)\n",
      "(49000, 28, 112, 1)\n",
      "(49000, 1, 5)\n",
      "(49000, 4, 10)\n",
      "(14000, 28, 112, 1)\n",
      "(14000, 1, 5)\n",
      "(14000, 4, 10)\n",
      "(7000, 28, 112, 1)\n",
      "(7000, 1, 5)\n",
      "(7000, 4, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(32, 28, 112, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(32, 5), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(32, 4, 10), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(32, 14, 56, 8), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(32, 7, 28, 16), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 25.488922\n",
      "Minibatch accuracy_length: 34.4%\n",
      "Minibatch accuracy_digit_0: 15.6%\n",
      "Minibatch accuracy_digit_1: 13.6%\n",
      "Minibatch accuracy_digit_2: 6.2%\n",
      "Minibatch loss at step 100: 7.210330\n",
      "Minibatch accuracy_length: 15.6%\n",
      "Minibatch accuracy_digit_0: 3.1%\n",
      "Minibatch accuracy_digit_1: 17.4%\n",
      "Minibatch accuracy_digit_2: 0.0%\n",
      "Minibatch loss at step 200: 7.183245\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 15.6%\n",
      "Minibatch accuracy_digit_1: 13.6%\n",
      "Minibatch accuracy_digit_2: 15.8%\n",
      "Minibatch loss at step 300: 7.479638\n",
      "Minibatch accuracy_length: 53.1%\n",
      "Minibatch accuracy_digit_0: 31.2%\n",
      "Minibatch accuracy_digit_1: 0.0%\n",
      "Minibatch accuracy_digit_2: 5.0%\n",
      "Minibatch loss at step 400: 5.435720\n",
      "Minibatch accuracy_length: 65.6%\n",
      "Minibatch accuracy_digit_0: 56.2%\n",
      "Minibatch accuracy_digit_1: 12.5%\n",
      "Minibatch accuracy_digit_2: 0.0%\n",
      "Minibatch loss at step 500: 5.212344\n",
      "Minibatch accuracy_length: 68.8%\n",
      "Minibatch accuracy_digit_0: 50.0%\n",
      "Minibatch accuracy_digit_1: 47.8%\n",
      "Minibatch accuracy_digit_2: 33.3%\n",
      "Minibatch loss at step 600: 4.640140\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 43.8%\n",
      "Minibatch accuracy_digit_1: 41.7%\n",
      "Minibatch accuracy_digit_2: 42.9%\n",
      "Minibatch loss at step 700: 4.053938\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 40.6%\n",
      "Minibatch accuracy_digit_1: 48.0%\n",
      "Minibatch accuracy_digit_2: 30.0%\n",
      "Minibatch loss at step 800: 4.482667\n",
      "Minibatch accuracy_length: 90.6%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 46.4%\n",
      "Minibatch accuracy_digit_2: 45.0%\n",
      "Minibatch loss at step 900: 3.453645\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 65.6%\n",
      "Minibatch accuracy_digit_1: 67.7%\n",
      "Minibatch accuracy_digit_2: 54.5%\n",
      "Minibatch loss at step 1000: 3.784208\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 42.3%\n",
      "Minibatch accuracy_digit_2: 45.0%\n",
      "Minibatch loss at step 1100: 3.249260\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 53.1%\n",
      "Minibatch accuracy_digit_1: 60.0%\n",
      "Minibatch accuracy_digit_2: 68.8%\n",
      "Minibatch loss at step 1200: 2.332102\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 68.8%\n",
      "Minibatch accuracy_digit_1: 78.3%\n",
      "Minibatch accuracy_digit_2: 62.5%\n",
      "Minibatch loss at step 1300: 2.847084\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 54.2%\n",
      "Minibatch accuracy_digit_2: 68.4%\n",
      "Minibatch loss at step 1400: 2.626020\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 62.5%\n",
      "Minibatch accuracy_digit_2: 81.2%\n",
      "Minibatch loss at step 1500: 2.531425\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 82.6%\n",
      "Minibatch accuracy_digit_2: 37.5%\n",
      "Minibatch loss at step 1600: 2.060742\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 75.0%\n",
      "Minibatch accuracy_digit_1: 86.4%\n",
      "Minibatch accuracy_digit_2: 64.7%\n",
      "Minibatch loss at step 1700: 2.183945\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 81.5%\n",
      "Minibatch accuracy_digit_2: 75.0%\n",
      "Minibatch loss at step 1800: 2.033103\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 87.5%\n",
      "Minibatch accuracy_digit_2: 64.7%\n",
      "Minibatch loss at step 1900: 1.631994\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 87.5%\n",
      "Minibatch accuracy_digit_2: 82.4%\n",
      "Minibatch loss at step 2000: 1.498737\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 69.2%\n",
      "Minibatch accuracy_digit_2: 92.9%\n",
      "Minibatch loss at step 2100: 1.981866\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 63.6%\n",
      "Minibatch accuracy_digit_2: 83.3%\n",
      "Minibatch loss at step 2200: 1.388371\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 91.3%\n",
      "Minibatch accuracy_digit_2: 66.7%\n",
      "Minibatch loss at step 2300: 1.603043\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 80.0%\n",
      "Minibatch accuracy_digit_2: 90.5%\n",
      "Minibatch loss at step 2400: 1.471492\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 88.5%\n",
      "Minibatch accuracy_digit_2: 75.0%\n",
      "Minibatch loss at step 2500: 1.685731\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 84.0%\n",
      "Minibatch accuracy_digit_2: 95.2%\n",
      "Minibatch loss at step 2600: 1.284748\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 79.2%\n",
      "Minibatch accuracy_digit_2: 82.4%\n",
      "Minibatch loss at step 2700: 1.761796\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 84.0%\n",
      "Minibatch accuracy_digit_2: 76.9%\n",
      "Minibatch loss at step 2800: 1.527269\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 80.0%\n",
      "Minibatch accuracy_digit_2: 76.9%\n",
      "Minibatch loss at step 2900: 1.206851\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 83.3%\n",
      "Minibatch accuracy_digit_2: 92.3%\n",
      "Minibatch loss at step 3000: 1.302532\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 86.4%\n",
      "Minibatch accuracy_digit_2: 100.0%\n",
      "Minibatch loss at step 3100: 0.949554\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 88.9%\n",
      "Minibatch accuracy_digit_2: 81.8%\n",
      "Minibatch loss at step 3200: 1.376128\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 90.5%\n",
      "Minibatch accuracy_digit_2: 82.4%\n",
      "Minibatch loss at step 3300: 1.188582\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 100.0%\n",
      "Minibatch accuracy_digit_2: 100.0%\n",
      "Minibatch loss at step 3400: 0.962970\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 100.0%\n",
      "Minibatch accuracy_digit_1: 89.3%\n",
      "Minibatch accuracy_digit_2: 94.4%\n",
      "Minibatch loss at step 3500: 1.355472\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 81.5%\n",
      "Minibatch accuracy_digit_2: 88.2%\n",
      "Minibatch loss at step 3600: 1.338001\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 83.3%\n",
      "Minibatch accuracy_digit_2: 81.2%\n",
      "Minibatch loss at step 3700: 0.975015\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 81.0%\n",
      "Minibatch accuracy_digit_2: 92.3%\n",
      "Minibatch loss at step 3800: 1.130363\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 84.6%\n",
      "Minibatch accuracy_digit_2: 100.0%\n",
      "Minibatch loss at step 3900: 1.040037\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 96.6%\n",
      "Minibatch accuracy_digit_2: 81.2%\n",
      "Minibatch loss at step 4000: 1.121994\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 81.8%\n",
      "Minibatch accuracy_digit_2: 84.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 8\n",
    "depth_1 = depth\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 2\n",
    "depth_4 = depth_3 * 2\n",
    "num_hidden = 48\n",
    "\n",
    "pooling_stride = 2\n",
    "pooling_kernel_size = 2\n",
    "beta = 0.001\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(4)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "#     print(conv)\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     reshape = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    reshape = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(reshape, digit_length_weights) + digit_length_biases    \n",
    "    digit_length_logit_inner = tf.to_float(tf.argmax(tf.nn.softmax(digit_length_logit), dimension=1))\n",
    "    \n",
    "    digits_0_logit = tf.matmul(reshape, digits_0_weights) + digits_0_biases\n",
    "    digits_1_logit = tf.matmul(reshape, digits_1_weights) + digits_1_biases\n",
    "    digits_2_logit = tf.matmul(reshape, digits_2_weights) + digits_2_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit, digits_1_logit, digits_2_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  digits_1_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 1))\n",
    "  digits_2_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 2))\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "        + digits_1_mult* tf.nn.softmax_cross_entropy_with_logits(logits[2], tf_train_digits_labels[:,1])\n",
    "        + digits_2_mult* tf.nn.softmax_cross_entropy_with_logits(logits[3], tf_train_digits_labels[:,2])\n",
    "        + beta * tf.nn.l2_loss(layer1_weights) \n",
    "        + beta * tf.nn.l2_loss(layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(fc_layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(digit_length_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_0_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_1_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_2_weights) \n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "  train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "  train_digits_2_prediction = tf.nn.softmax(logits[3])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0, p_1, p_2 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction, train_digits_2_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,3):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 : 숫자 4개\n",
    "- 출력 : 숫자 4개 + 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 3136)\n",
      "(70000, 1)\n",
      "(70000, 4)\n",
      "(49000, 28, 112, 1)\n",
      "(49000, 1, 5)\n",
      "(49000, 4, 10)\n",
      "(14000, 28, 112, 1)\n",
      "(14000, 1, 5)\n",
      "(14000, 4, 10)\n",
      "(7000, 28, 112, 1)\n",
      "(7000, 1, 5)\n",
      "(7000, 4, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(32, 28, 112, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(32, 5), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(32, 4, 10), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(32, 14, 56, 8), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(32, 7, 28, 16), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 37.588093\n",
      "Minibatch accuracy_length: 18.8%\n",
      "Minibatch accuracy_digit_0: 9.4%\n",
      "Minibatch accuracy_digit_1: 7.7%\n",
      "Minibatch accuracy_digit_2: 13.3%\n",
      "Minibatch accuracy_digit_3: 0.0%\n",
      "Minibatch loss at step 100: 7.331507\n",
      "Minibatch accuracy_length: 81.2%\n",
      "Minibatch accuracy_digit_0: 6.2%\n",
      "Minibatch accuracy_digit_1: 19.2%\n",
      "Minibatch accuracy_digit_2: 0.0%\n",
      "Minibatch accuracy_digit_3: 14.3%\n",
      "Minibatch loss at step 200: 6.182780\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 18.8%\n",
      "Minibatch accuracy_digit_1: 26.1%\n",
      "Minibatch accuracy_digit_2: 13.3%\n",
      "Minibatch accuracy_digit_3: 20.0%\n",
      "Minibatch loss at step 300: 6.796660\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch accuracy_digit_1: 22.7%\n",
      "Minibatch accuracy_digit_2: 16.7%\n",
      "Minibatch accuracy_digit_3: 0.0%\n",
      "Minibatch loss at step 400: 6.533216\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 28.1%\n",
      "Minibatch accuracy_digit_1: 16.7%\n",
      "Minibatch accuracy_digit_2: 5.9%\n",
      "Minibatch accuracy_digit_3: 9.1%\n",
      "Minibatch loss at step 500: 5.260745\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 21.9%\n",
      "Minibatch accuracy_digit_1: 39.1%\n",
      "Minibatch accuracy_digit_2: 37.5%\n",
      "Minibatch accuracy_digit_3: 40.0%\n",
      "Minibatch loss at step 600: 5.529148\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 31.2%\n",
      "Minibatch accuracy_digit_1: 24.0%\n",
      "Minibatch accuracy_digit_2: 29.4%\n",
      "Minibatch accuracy_digit_3: 20.0%\n",
      "Minibatch loss at step 700: 4.754255\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 50.0%\n",
      "Minibatch accuracy_digit_1: 28.0%\n",
      "Minibatch accuracy_digit_2: 25.0%\n",
      "Minibatch accuracy_digit_3: 12.5%\n",
      "Minibatch loss at step 800: 4.839853\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 46.9%\n",
      "Minibatch accuracy_digit_1: 52.0%\n",
      "Minibatch accuracy_digit_2: 20.0%\n",
      "Minibatch accuracy_digit_3: 37.5%\n",
      "Minibatch loss at step 900: 3.888192\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 59.4%\n",
      "Minibatch accuracy_digit_1: 54.5%\n",
      "Minibatch accuracy_digit_2: 40.0%\n",
      "Minibatch accuracy_digit_3: 40.0%\n",
      "Minibatch loss at step 1000: 4.323393\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 56.2%\n",
      "Minibatch accuracy_digit_1: 52.0%\n",
      "Minibatch accuracy_digit_2: 50.0%\n",
      "Minibatch accuracy_digit_3: 22.2%\n",
      "Minibatch loss at step 1100: 3.566528\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 53.1%\n",
      "Minibatch accuracy_digit_1: 63.6%\n",
      "Minibatch accuracy_digit_2: 15.4%\n",
      "Minibatch accuracy_digit_3: 0.0%\n",
      "Minibatch loss at step 1200: 3.599703\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 75.9%\n",
      "Minibatch accuracy_digit_2: 31.6%\n",
      "Minibatch accuracy_digit_3: 27.3%\n",
      "Minibatch loss at step 1300: 3.949345\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 62.5%\n",
      "Minibatch accuracy_digit_1: 78.6%\n",
      "Minibatch accuracy_digit_2: 52.6%\n",
      "Minibatch accuracy_digit_3: 40.0%\n",
      "Minibatch loss at step 1400: 3.614252\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 71.9%\n",
      "Minibatch accuracy_digit_1: 56.0%\n",
      "Minibatch accuracy_digit_2: 52.6%\n",
      "Minibatch accuracy_digit_3: 61.5%\n",
      "Minibatch loss at step 1500: 2.990292\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 77.8%\n",
      "Minibatch accuracy_digit_2: 73.7%\n",
      "Minibatch accuracy_digit_3: 58.3%\n",
      "Minibatch loss at step 1600: 2.740574\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 75.0%\n",
      "Minibatch accuracy_digit_2: 43.8%\n",
      "Minibatch accuracy_digit_3: 71.4%\n",
      "Minibatch loss at step 1700: 3.004620\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 59.3%\n",
      "Minibatch accuracy_digit_2: 70.6%\n",
      "Minibatch accuracy_digit_3: 41.7%\n",
      "Minibatch loss at step 1800: 2.325884\n",
      "Minibatch accuracy_length: 96.9%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 75.0%\n",
      "Minibatch accuracy_digit_2: 86.7%\n",
      "Minibatch accuracy_digit_3: 16.7%\n",
      "Minibatch loss at step 1900: 3.567466\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 66.7%\n",
      "Minibatch accuracy_digit_2: 55.0%\n",
      "Minibatch accuracy_digit_3: 58.3%\n",
      "Minibatch loss at step 2000: 2.681807\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 71.9%\n",
      "Minibatch accuracy_digit_1: 83.3%\n",
      "Minibatch accuracy_digit_2: 56.2%\n",
      "Minibatch accuracy_digit_3: 62.5%\n",
      "Minibatch loss at step 2100: 2.407517\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 81.2%\n",
      "Minibatch accuracy_digit_1: 72.7%\n",
      "Minibatch accuracy_digit_2: 84.6%\n",
      "Minibatch accuracy_digit_3: 50.0%\n",
      "Minibatch loss at step 2200: 1.693584\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 90.0%\n",
      "Minibatch accuracy_digit_2: 50.0%\n",
      "Minibatch accuracy_digit_3: 75.0%\n",
      "Minibatch loss at step 2300: 2.010534\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 84.4%\n",
      "Minibatch accuracy_digit_1: 78.3%\n",
      "Minibatch accuracy_digit_2: 83.3%\n",
      "Minibatch accuracy_digit_3: 75.0%\n",
      "Minibatch loss at step 2400: 1.899512\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 92.0%\n",
      "Minibatch accuracy_digit_2: 78.9%\n",
      "Minibatch accuracy_digit_3: 80.0%\n",
      "Minibatch loss at step 2500: 2.462399\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 76.9%\n",
      "Minibatch accuracy_digit_2: 62.5%\n",
      "Minibatch accuracy_digit_3: 55.6%\n",
      "Minibatch loss at step 2600: 1.926757\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 69.6%\n",
      "Minibatch accuracy_digit_2: 83.3%\n",
      "Minibatch accuracy_digit_3: 66.7%\n",
      "Minibatch loss at step 2700: 1.962816\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 77.3%\n",
      "Minibatch accuracy_digit_2: 64.3%\n",
      "Minibatch accuracy_digit_3: 62.5%\n",
      "Minibatch loss at step 2800: 1.851913\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 92.0%\n",
      "Minibatch accuracy_digit_2: 78.9%\n",
      "Minibatch accuracy_digit_3: 80.0%\n",
      "Minibatch loss at step 2900: 2.211033\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 65.6%\n",
      "Minibatch accuracy_digit_1: 68.0%\n",
      "Minibatch accuracy_digit_2: 100.0%\n",
      "Minibatch accuracy_digit_3: 83.3%\n",
      "Minibatch loss at step 3000: 2.960391\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 47.8%\n",
      "Minibatch accuracy_digit_2: 64.7%\n",
      "Minibatch accuracy_digit_3: 83.3%\n",
      "Minibatch loss at step 3100: 2.214529\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 80.8%\n",
      "Minibatch accuracy_digit_2: 73.7%\n",
      "Minibatch accuracy_digit_3: 80.0%\n",
      "Minibatch loss at step 3200: 2.021976\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 82.1%\n",
      "Minibatch accuracy_digit_2: 70.6%\n",
      "Minibatch accuracy_digit_3: 75.0%\n",
      "Minibatch loss at step 3300: 1.786043\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 85.2%\n",
      "Minibatch accuracy_digit_2: 88.2%\n",
      "Minibatch accuracy_digit_3: 63.6%\n",
      "Minibatch loss at step 3400: 1.424156\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 90.6%\n",
      "Minibatch accuracy_digit_1: 100.0%\n",
      "Minibatch accuracy_digit_2: 85.7%\n",
      "Minibatch accuracy_digit_3: 71.4%\n",
      "Minibatch loss at step 3500: 1.888120\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 87.5%\n",
      "Minibatch accuracy_digit_1: 79.2%\n",
      "Minibatch accuracy_digit_2: 100.0%\n",
      "Minibatch accuracy_digit_3: 60.0%\n",
      "Minibatch loss at step 3600: 2.124707\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 75.0%\n",
      "Minibatch accuracy_digit_2: 85.7%\n",
      "Minibatch accuracy_digit_3: 75.0%\n",
      "Minibatch loss at step 3700: 1.937475\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 88.9%\n",
      "Minibatch accuracy_digit_2: 64.7%\n",
      "Minibatch accuracy_digit_3: 75.0%\n",
      "Minibatch loss at step 3800: 1.446835\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 93.8%\n",
      "Minibatch accuracy_digit_1: 100.0%\n",
      "Minibatch accuracy_digit_2: 89.5%\n",
      "Minibatch accuracy_digit_3: 84.6%\n",
      "Minibatch loss at step 3900: 1.243776\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 96.9%\n",
      "Minibatch accuracy_digit_1: 100.0%\n",
      "Minibatch accuracy_digit_2: 88.2%\n",
      "Minibatch accuracy_digit_3: 100.0%\n",
      "Minibatch loss at step 4000: 1.620113\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 78.1%\n",
      "Minibatch accuracy_digit_1: 88.9%\n",
      "Minibatch accuracy_digit_2: 88.2%\n",
      "Minibatch accuracy_digit_3: 90.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "depth = 8\n",
    "depth_1 = depth\n",
    "depth_2 = depth_1 * 2\n",
    "depth_3 = depth_2 * 2\n",
    "depth_4 = depth_3 * 2\n",
    "num_hidden = 64\n",
    "\n",
    "pooling_stride = 2\n",
    "pooling_kernel_size = 2\n",
    "beta = 0.001\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(4)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "    conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    if is_training:\n",
    "        conv = tf.nn.dropout(conv, 0.8)\n",
    "    print(conv)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "#     print(conv)\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "    \n",
    "#     conv = tf.nn.conv2d(conv, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer4_biases)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     reshape = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    reshape = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "\n",
    "    digit_length_logit = tf.matmul(reshape, digit_length_weights) + digit_length_biases    \n",
    "    digit_length_logit_inner = tf.to_float(tf.argmax(tf.nn.softmax(digit_length_logit), dimension=1))\n",
    "    \n",
    "    digits_0_logit = tf.matmul(reshape, digits_0_weights) + digits_0_biases\n",
    "    digits_1_logit = tf.matmul(reshape, digits_1_weights) + digits_1_biases\n",
    "    digits_2_logit = tf.matmul(reshape, digits_2_weights) + digits_2_biases\n",
    "    digits_3_logit = tf.matmul(reshape, digits_3_weights) + digits_3_biases\n",
    "\n",
    "    return digit_length_logit, digits_0_logit, digits_1_logit, digits_2_logit, digits_3_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  digits_1_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 1))\n",
    "  digits_2_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 2))\n",
    "  digits_3_mult = tf.to_float((tf.argmax(tf.nn.softmax(tf_train_length_labels), dimension=1) > 3))\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "        + tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "        + digits_1_mult* tf.nn.softmax_cross_entropy_with_logits(logits[2], tf_train_digits_labels[:,1])\n",
    "        + digits_2_mult* tf.nn.softmax_cross_entropy_with_logits(logits[3], tf_train_digits_labels[:,2])\n",
    "        + digits_3_mult* tf.nn.softmax_cross_entropy_with_logits(logits[4], tf_train_digits_labels[:,3])\n",
    "        + beta * tf.nn.l2_loss(layer1_weights) \n",
    "        + beta * tf.nn.l2_loss(layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(fc_layer2_weights) \n",
    "        + beta * tf.nn.l2_loss(digit_length_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_0_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_1_weights) \n",
    "        + beta * tf.nn.l2_loss(digits_2_weights) \n",
    "    )\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "#   global_step = tf.Variable(0, trainable=False)\n",
    "#   starter_learning_rate = 0.1\n",
    "#   learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100, 0.96, staircase=True)\n",
    "#   optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "  train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "  train_digits_2_prediction = tf.nn.softmax(logits[3])\n",
    "  train_digits_3_prediction = tf.nn.softmax(logits[4])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0, p_1, p_2, p_3 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction, train_digits_2_prediction, train_digits_3_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,4):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1)\n",
      "(70000, 1)\n",
      "(49000, 28, 28, 1)\n",
      "(49000, 1, 2)\n",
      "(49000, 1, 10)\n",
      "(14000, 28, 28, 1)\n",
      "(14000, 1, 2)\n",
      "(14000, 1, 10)\n",
      "(7000, 28, 28, 1)\n",
      "(7000, 1, 2)\n",
      "(7000, 1, 10)\n",
      "tf_train_dataset : Tensor(\"Placeholder:0\", shape=(16, 28, 28, 1), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(16, 2), dtype=float32)\n",
      "tf_train_digits_labels : Tensor(\"Placeholder_2:0\", shape=(16, 1, 10), dtype=float32)\n",
      "layer1 : Tensor(\"Relu:0\", shape=(16, 14, 14, 16), dtype=float32)\n",
      "layer2 : Tensor(\"Relu_1:0\", shape=(16, 7, 7, 16), dtype=float32)\n",
      "reshape : Tensor(\"Reshape:0\", shape=(16, 784), dtype=float32)\n",
      "fc_layer1_weights : (784, 784)\n",
      "fc_layer1 : Tensor(\"Relu_1:0\", shape=(16, 7, 7, 16), dtype=float32)\n",
      "fc_layer2 : Tensor(\"Relu_2:0\", shape=(16, 64), dtype=float32)\n",
      "digit length layer : Tensor(\"add_3:0\", shape=(16, 2), dtype=float32)\n",
      "digit_length_logit_inner : Tensor(\"ToFloat:0\", shape=(16,), dtype=float32)\n",
      "digit[0] layer : Tensor(\"add_4:0\", shape=(16, 10), dtype=float32)\n",
      "digits_1_mult_packed : Tensor(\"transpose:0\", shape=(16, 10), dtype=float32)\n",
      "digit[1] layer : Tensor(\"mul:0\", shape=(16, 10), dtype=float32)\n",
      "digits_2_mult_packed : Tensor(\"transpose_1:0\", shape=(16, 10), dtype=float32)\n",
      "digit[2] layer : Tensor(\"mul_1:0\", shape=(16, 10), dtype=float32)\n",
      "digits_3_mult_packed : Tensor(\"transpose_2:0\", shape=(16, 10), dtype=float32)\n",
      "digit[3] layer : Tensor(\"mul_2:0\", shape=(16, 10), dtype=float32)\n",
      "digits_4_mult_packed : Tensor(\"transpose_3:0\", shape=(16, 10), dtype=float32)\n",
      "digit[4] layer : Tensor(\"mul_3:0\", shape=(16, 10), dtype=float32)\n",
      "logits[0] : Tensor(\"add_3:0\", shape=(16, 2), dtype=float32)\n",
      "tf_train_length_labels : Tensor(\"Placeholder_1:0\", shape=(16, 2), dtype=float32)\n",
      "logits[1] : Tensor(\"add_4:0\", shape=(16, 10), dtype=float32)\n",
      "tf_train_digits_labels[:,0] : Tensor(\"strided_slice_1:0\", shape=(16, 10), dtype=float32)\n",
      "train_length_prediction : Tensor(\"Softmax_1:0\", shape=(16, 2), dtype=float32)\n",
      "train_digits_0_prediction : Tensor(\"Softmax_2:0\", shape=(16, 10), dtype=float32)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 4.501227\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 6.2%\n",
      "Minibatch loss at step 50: 2.292343\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 25.0%\n",
      "Minibatch loss at step 100: 2.261173\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 12.5%\n",
      "Minibatch loss at step 150: 2.809151\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 6.2%\n",
      "Minibatch loss at step 200: 2.224278\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 31.2%\n",
      "Minibatch loss at step 250: 2.396038\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 18.8%\n",
      "Minibatch loss at step 300: 2.468648\n",
      "Minibatch accuracy_length: 100.0%\n",
      "Minibatch accuracy_digit_0: 6.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 301\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "num_channels = 1\n",
    "depth = 16\n",
    "depth_1 = 16\n",
    "depth_2 = 16\n",
    "depth_3 = 16\n",
    "num_hidden = 64\n",
    "\n",
    "pooling_stride = 2\n",
    "pooling_kernel_size = 2\n",
    "beta = 0.0000001\n",
    "\n",
    "multi_digit_mnist = MultiDigitMNISTData(1)\n",
    "multi_digit_mnist.load_data()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.image_width, multi_digit_mnist.image_height, multi_digit_mnist.num_channels))\n",
    "  print(\"tf_train_dataset : {}\".format(tf_train_dataset))\n",
    "  tf_train_length_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count + 1))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  tf_train_digits_labels = tf.placeholder(tf.float32, shape=(batch_size, multi_digit_mnist.digit_count, 10))\n",
    "  print(\"tf_train_digits_labels : {}\".format(tf_train_digits_labels))\n",
    "  tf_valid_dataset = tf.constant(multi_digit_mnist.validation_data)\n",
    "  tf_test_dataset = tf.constant(multi_digit_mnist.test_data)\n",
    "\n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  layer1_biases =  tf.Variable(tf.constant(1.0, shape=[depth_1]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "\n",
    "  flatted=int(round(float(multi_digit_mnist.image_width) / (pooling_stride ** 2)) * round(float(multi_digit_mnist.image_height) / (pooling_stride ** 2)) * depth_2)\n",
    "  fc_layer1_weights = tf.Variable(tf.truncated_normal([flatted, flatted], stddev=0.1))\n",
    "  fc_layer1_biases = tf.Variable(tf.constant(1.0, shape=[flatted]))\n",
    "\n",
    "  fc_layer2_weights = tf.Variable(tf.truncated_normal([flatted, num_hidden], stddev=0.1))\n",
    "  fc_layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  digit_length_weights = tf.Variable(tf.truncated_normal([num_hidden, multi_digit_mnist.digit_count + 1], stddev=0.1))\n",
    "  digit_length_biases = tf.Variable(tf.constant(1.0, shape=[multi_digit_mnist.digit_count + 1]))\n",
    "\n",
    "  digits_0_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_0_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_1_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_1_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_2_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_2_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_3_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_3_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  digits_4_weights = tf.Variable(tf.truncated_normal([num_hidden, 10], stddev=0.1))\n",
    "  digits_4_biases = tf.Variable(tf.constant(1.0, shape=[10]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data, is_training=True):\n",
    "    stride = 2\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, stride, stride, 1], padding='SAME')\n",
    "#     conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer1_biases)\n",
    "#     conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    print(\"layer1 : {}\".format(conv)) # (70, 70, 14, 16)\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "\n",
    "    conv = tf.nn.conv2d(conv, layer2_weights, [1, stride, stride, 1], padding='SAME')\n",
    "#     conv = tf.nn.conv2d(conv, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv + layer2_biases)\n",
    "#     conv = tf.nn.max_pool(conv, [1, stride, stride, 1], [1, stride, stride, 1], padding='SAME')\n",
    "    print(\"layer2 : {}\".format(conv)) # (70, 35, 7, 16)\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "\n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, stride, stride, 1], padding='SAME')\n",
    "#     conv = tf.nn.conv2d(conv, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     conv = tf.nn.relu(conv + layer3_biases)\n",
    "#     conv = tf.nn.max_pool(conv, [1, stride, 1, 1], [1, stride, 1, 1], padding='SAME')\n",
    "#     print(\"layer3 : {}\".format(conv))\n",
    "#     if is_training:\n",
    "#         conv = tf.nn.dropout(conv, 0.8)\n",
    "\n",
    "    shape = conv.get_shape().as_list()\n",
    "    reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    # (32, 1152) = 18 * 4 * 16\n",
    "    print(\"reshape : {}\".format(reshape))\n",
    "    # (944, 256) = 17 * 3 * 16\n",
    "    print(\"fc_layer1_weights : {}\".format(fc_layer1_weights.get_shape()))\n",
    "#     conv = tf.nn.relu(tf.matmul(reshape, fc_layer1_weights) + fc_layer1_biases)\n",
    "    print(\"fc_layer1 : {}\".format(conv)) # (70, 64)\n",
    "    conv = tf.nn.relu(tf.matmul(reshape, fc_layer2_weights) + fc_layer2_biases)\n",
    "    print(\"fc_layer2 : {}\".format(conv)) # (70, 64)\n",
    "\n",
    "    digit_length_logit = tf.matmul(conv, digit_length_weights) + digit_length_biases\n",
    "    print(\"digit length layer : {}\".format(digit_length_logit)) # (16, 6)\n",
    "    \n",
    "    digit_length_logit_inner = tf.to_float(tf.argmax(tf.nn.softmax(digit_length_logit), dimension=1))\n",
    "    print(\"digit_length_logit_inner : {}\".format(digit_length_logit_inner)) # (16, )\n",
    "\n",
    "    digits_0_logit = tf.matmul(conv, digits_0_weights) + digits_0_biases\n",
    "    print(\"digit[0] layer : {}\".format(digits_0_logit)) # (16, 10)\n",
    "    \n",
    "    digits_1_mult = tf.maximum(digit_length_logit_inner - 1, 0)\n",
    "    digits_1_mult_packed = tf.transpose(tf.pack([digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult,digits_1_mult]))\n",
    "    print(\"digits_1_mult_packed : {}\".format(digits_1_mult_packed)) # (16, 10)\n",
    "    digits_1_logit = (tf.matmul(conv, digits_1_weights) + digits_1_biases) * digits_1_mult_packed\n",
    "    print(\"digit[1] layer : {}\".format(digits_1_logit)) # (16, 10)\n",
    "    \n",
    "    digits_2_mult = tf.maximum(digit_length_logit_inner - 2, 0)\n",
    "    digits_2_mult_packed = tf.transpose(tf.pack([digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult,digits_2_mult]))\n",
    "    print(\"digits_2_mult_packed : {}\".format(digits_2_mult_packed)) # (16, 10)\n",
    "    digits_2_logit = (tf.matmul(conv, digits_2_weights) + digits_2_biases) * digits_2_mult_packed\n",
    "    print(\"digit[2] layer : {}\".format(digits_2_logit)) # (16, 10)\n",
    "    \n",
    "    digits_3_mult = tf.maximum(digit_length_logit_inner - 3, 0)\n",
    "    digits_3_mult_packed = tf.transpose(tf.pack([digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult,digits_3_mult]))\n",
    "    print(\"digits_3_mult_packed : {}\".format(digits_3_mult_packed)) # (16, 10)\n",
    "    digits_3_logit = (tf.matmul(conv, digits_3_weights) + digits_3_biases) * digits_3_mult_packed\n",
    "    print(\"digit[3] layer : {}\".format(digits_3_logit)) # (16, 10)\n",
    "    \n",
    "    digits_4_mult = tf.maximum(digit_length_logit_inner - 4, 0)\n",
    "    digits_4_mult_packed = tf.transpose(tf.pack([digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult,digits_4_mult]))\n",
    "    print(\"digits_4_mult_packed : {}\".format(digits_4_mult_packed)) # (16, 10)\n",
    "    digits_4_logit = (tf.matmul(conv, digits_4_weights) + digits_4_biases) * digits_4_mult_packed\n",
    "    print(\"digit[4] layer : {}\".format(digits_4_logit)) # (16, 10)\n",
    "\n",
    "    return digit_length_logit,digits_0_logit,digits_1_logit,digits_2_logit,digits_3_logit,digits_4_logit;\n",
    "\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "#                         tf.nn.softmax_cross_entropy_with_logits(logits[0], tf_train_length_labels)\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(logits[1], tf_train_digits_labels[:,0])\n",
    "#                         + tf.nn.softmax_cross_entropy_with_logits(logits[2], tf_train_digits_labels[:,1])\n",
    "#                         + tf.nn.softmax_cross_entropy_with_logits(logits[3], tf_train_digits_labels[:,2])\n",
    "#                         + tf.nn.softmax_cross_entropy_with_logits(logits[4], tf_train_digits_labels[:,3])\n",
    "#                         + tf.nn.softmax_cross_entropy_with_logits(logits[5], tf_train_digits_labels[:,4])\n",
    "#                         + beta * tf.nn.l2_loss(layer1_weights) \n",
    "#                         + beta * tf.nn.l2_loss(layer2_weights) \n",
    "#                         + beta * tf.nn.l2_loss(layer3_weights) \n",
    "#                         + beta * tf.nn.l2_loss(fc_layer1_weights) \n",
    "#                         + beta * tf.nn.l2_loss(fc_layer2_weights) \n",
    "#                         + beta * tf.nn.l2_loss(digits_0_weights) \n",
    "#                         + beta * tf.nn.l2_loss(digits_1_weights) \n",
    "#                         + beta * tf.nn.l2_loss(digits_2_weights) \n",
    "#                         + beta * tf.nn.l2_loss(digits_3_weights) \n",
    "#                         + beta * tf.nn.l2_loss(digits_4_weights) \n",
    "#                         + beta * tf.nn.l2_loss(digit_length_weights) \n",
    "                       )\n",
    "\n",
    "\n",
    "  print(\"logits[0] : {}\".format(logits[0]))\n",
    "  print(\"tf_train_length_labels : {}\".format(tf_train_length_labels))\n",
    "  print(\"logits[1] : {}\".format(logits[1]))\n",
    "  print(\"tf_train_digits_labels[:,0] : {}\".format(tf_train_digits_labels[:,0]))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  starter_learning_rate = 0.1\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100, 0.96, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_length_prediction = tf.nn.softmax(logits[0])\n",
    "  train_digits_0_prediction = tf.nn.softmax(logits[1])\n",
    "  train_digits_1_prediction = tf.nn.softmax(logits[2])\n",
    "  train_digits_2_prediction = tf.nn.softmax(logits[3])\n",
    "  train_digits_3_prediction = tf.nn.softmax(logits[4])\n",
    "  train_digits_4_prediction = tf.nn.softmax(logits[5])\n",
    "\n",
    "  print(\"train_length_prediction : {}\".format(train_length_prediction))\n",
    "  print(\"train_digits_0_prediction : {}\".format(train_digits_0_prediction))\n",
    "\n",
    "  #valid_prediction = tf.argmax(model(multi_digit_mnist.validation_data, False), 1)\n",
    "  #test_prediction = tf.argmax(model(multi_digit_mnist.test_data, False), 1)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (multi_digit_mnist.train_data.shape[0] - batch_size)\n",
    "    batch_data = multi_digit_mnist.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_length_labels = multi_digit_mnist.train_label_length[offset:(offset + batch_size), 0, :]\n",
    "    batch_digits_labels = multi_digit_mnist.train_label_digits[offset:(offset + batch_size), :]\n",
    "    \n",
    "#     print(batch_data.shape)\n",
    "#     print(batch_length_labels.shape)\n",
    "#     print(batch_digits_labels.shape)\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_length_labels : batch_length_labels, tf_train_digits_labels : batch_digits_labels}\n",
    "    _, l, p_length, p_0, p_1, p_2, p_3, p_4 = session.run(\n",
    "      [optimizer, loss, train_length_prediction, train_digits_0_prediction, train_digits_1_prediction, train_digits_2_prediction, train_digits_3_prediction, train_digits_4_prediction], feed_dict=feed_dict)\n",
    "#     print('Minibatch loss at step %d: %f' % (step, l))\n",
    "#     print(batch_digits_labels[:,0])\n",
    "#     print(p_0)\n",
    "#     print(\"digits_0_weights.l2_loss : {}\".format(tf.nn.l2_loss(digits_0_weights).eval()))\n",
    "#     print(\"digits_0_biases.l2_loss : {}\".format(tf.nn.l2_loss(digits_0_biases).eval()))\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      #accuracy_result = accuracy(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      accuracy_result = accuracy_length(p_length, p_0, p_1, p_2, p_3, p_4, batch_length_labels, batch_digits_labels)\n",
    "      print('Minibatch accuracy_length: %.1f%%' % accuracy_result)\n",
    "      for k in range(0,1):\n",
    "          accuracy_result = accuracy_digit(p_length, [p_0, p_1, p_2, p_3, p_4], batch_length_labels, batch_digits_labels, k)\n",
    "          print(\"Minibatch accuracy_digit_{}\".format(k) + \": %.1f%%\" % accuracy_result)\n",
    "      #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train a model on a realistic dataset.\n",
    "\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, [the SVHN dataset](https://www.google.com/url?q=http://ufldl.stanford.edu/housenumbers/&sa=D&ust=1476236462762000&usg=AFQjCNGwcYwoDgoe0HcsyDz68YlW8fMgbA) is a good large scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to do well.\n",
    "\n",
    "- _How does your model perform on a realistic dataset?_\n",
    "- _What changes did you have to make, if any?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (optional): Put the model into an Android app.\n",
    "\n",
    "Do this step only if you have access to an Android device. If you don’t, you may either:\n",
    "- i) take pictures of numbers that you find around you, and run them through your classifier on your computer to produce example results, or,\n",
    "- ii) use OpenCV / SimpleCV / Pygame to capture live images from a webcam.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in [the TensorFlow Android demo app](https://www.google.com/url?q=https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android&sa=D&ust=1476236462766000&usg=AFQjCNHmG7Hq8LBapwPWMAshjdU7AwABwA), which you can simply modify.\n",
    "\n",
    "- _Is your model able to perform equally well on captured pictures or a live camera stream?_\n",
    "- _Document how you built the interface to your model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore!\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Simply training a regression loss to the coordinates of the bounding box is one way to get decent localization.\n",
    "\n",
    "Once you have the data localized, you can for example try turn it into an augmented reality app by overlaying your answer on the image like the Word Lens app does.\n",
    "\n",
    "Those are just examples of extensions you can look into. Use your imagination!\n",
    "\n",
    "- _Make sure to report what extension(s) you have implemented and how they worked._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
